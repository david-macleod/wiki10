{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a document classifier using the built-in spaCy model (CNN). Code is modified from textcategorizer example in spaCy docs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "from itertools import islice \n",
    "from sklearn import metrics\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('../../data/wiki10')\n",
    "TEXT_DIR = DATA_DIR / 'text' \n",
    "CLF_LABELS_PATH = DATA_DIR / 'clf0-singlelabel.csv'\n",
    "MODEL_DIR = DATA_DIR / 'model' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(n_texts, split=0.7):\n",
    "    # load labels\n",
    "    CLF_LABELS_PATH = DATA_DIR / 'clf0-singlelabel.csv'\n",
    "    df = pd.read_csv(CLF_LABELS_PATH).sample(frac=1, random_state=0) # load and shuffle records\n",
    "    df = df[:n_texts] \n",
    "    labels = df.tag.unique()\n",
    "    cats = [{label.upper(): label == tag for label in labels} for tag in df.tag]\n",
    "    # load texts\n",
    "    texts = [TEXT_DIR.joinpath(id).read_text() for id in df.id]\n",
    "    # split into train/test sets \n",
    "    split = int(len(df) * split)\n",
    "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 1e-8  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 1e-8  # True negatives\n",
    "    \n",
    "    for i, doc in enumerate(textcat.pipe(docs, n_threads=1, batch_size=20)): # fixing n_threads to avoid memory error\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(nlp, n_iter, output_dir, n_texts=2000):\n",
    "    # add the text classifier to the pipeline if it doesn't exist\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'textcat' not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe('textcat')\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "    # add labels to text classifier\n",
    "    textcat.add_label('MUSIC')\n",
    "    textcat.add_label('FOOD')\n",
    "    textcat.add_label('RELIGION')\n",
    "    textcat.add_label('SOFTWARE')\n",
    "    textcat.add_label('POLITICS')\n",
    "    textcat.add_label('MATH')\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    (train_texts, train_cats), (test_texts, test_cats) = load_data(n_texts=n_texts)\n",
    "    print(\"Loaded examples ({} training, {} evaluation)\"\n",
    "          .format(len(train_texts), len(test_texts)))\n",
    "    train_data = list(zip(train_texts,\n",
    "                          [{'cats': cats} for cats in train_cats]))\n",
    "    \n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.begin_training()\n",
    "        print(\"Training the model...\")\n",
    "        for i in range(n_iter):\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "            for i_batch, batch in enumerate(batches):\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "            # evaluate on the dev data split off in load_data\n",
    "            true_labels, pred_labels = get_labels(nlp.tokenizer, textcat, test_texts, test_cats)\n",
    "            print(metrics.classification_report(true_labels, pred_labels))\n",
    "            print(f\"LOSS: {losses['textcat']:.3f}\")\n",
    "            \n",
    "    # test the trained model\n",
    "    #test_text = \"Grapes are red or green\"\n",
    "    #doc = nlp(test_text)\n",
    "    #print(test_text, doc.cats)\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded examples (1260 training, 540 evaluation)\n",
      "Training the model...\n",
      "LOSS \t  P  \t  R  \t  F  \n",
      "140.330\t0.942\t0.757\t0.840\n",
      "70.210\t0.955\t0.746\t0.838\n",
      "Saved model to ../../data/wiki10/model\n",
      "CPU times: user 15min 3s, sys: 8.12 s, total: 15min 11s\n",
      "Wall time: 15min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser', 'tagger', 'ner']) \n",
    "main(nlp, n_iter=2, output_dir=MODEL_DIR, n_texts=1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded examples (1260 training, 540 evaluation)\n",
      "Training the model...\n",
      "LOSS \t  P  \t  R  \t  F  \n",
      "196.336\t0.962\t0.426\t0.591\n",
      "158.882\t0.959\t0.433\t0.597\n",
      "Saved model to ../../data/wiki10/model\n",
      "CPU times: user 15min 1s, sys: 6.73 s, total: 15min 8s\n",
      "Wall time: 15min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser', 'tagger', 'ner']) \n",
    "main(nlp, n_iter=2, output_dir=MODEL_DIR, n_texts=1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that simply re-running the model can produce a very different result, in terms of performance. As we have fixed the random_state of the training sample, the only thing (I think!) which could make the result not deterministic is the batches used to train the model. Will investigate this. \n",
    "\n",
    "Also note the difference in precision and recall, this is likely due to the evaluation process. It is set up to handle multilabel data (n-hot encoded target) but we are using a single label for each document. The metric considers each label for each document as a separate outcome so in our case of 6 categories, a correct prediction for a single document which actually be scored as 6 correct predictions (assuming the probability for the true class is >= 0.5 and the probability for all the false classes < 0.5).\n",
    "\n",
    "Changing the evaluation to consider each output class separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(tokenizer, textcat, texts, cats):\n",
    "    ''' output list of true labels and predicted labels for sklearn.evaluate input '''\n",
    "    docs = (tokenizer(text) for text in texts) # tokenizing/textcat outside of pipeline to allow custom weights (moving average)\n",
    "    \n",
    "    true_labels = [max(cat_dict, key=cat_dict.get) for cat_dict in cats]\n",
    "    pred_labels = [max(doc.cats, key=doc.cats.get) for doc in textcat.pipe(docs, n_threads=1, batch_size=20)]\n",
    "    \n",
    "    return true_labels, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded examples (1260 training, 540 evaluation)\n",
      "Training the model...\n",
      "LOSS \t  P  \t  R  \t  F  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       FOOD       0.00      0.00      0.00        93\n",
      "       MATH       0.80      0.97      0.88        79\n",
      "      MUSIC       0.46      0.96      0.62        99\n",
      "   POLITICS       0.00      0.00      0.00        88\n",
      "   RELIGION       0.72      0.98      0.83        95\n",
      "   SOFTWARE       0.77      0.95      0.85        86\n",
      "\n",
      "avg / total       0.45      0.64      0.52       540\n",
      "\n",
      "LOSS: 118.999\n",
      "Saved model to ../../data/wiki10/model\n",
      "CPU times: user 14min 1s, sys: 11.2 s, total: 14min 12s\n",
      "Wall time: 14min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser', 'tagger', 'ner']) \n",
    "main(nlp, n_iter=2, output_dir=MODEL_DIR, n_texts=1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded examples (1260 training, 540 evaluation)\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       FOOD       0.00      0.00      0.00        93\n",
      "       MATH       0.76      0.99      0.86        79\n",
      "      MUSIC       0.64      0.93      0.76        99\n",
      "   POLITICS       0.92      0.95      0.94        88\n",
      "   RELIGION       0.94      0.95      0.94        95\n",
      "   SOFTWARE       0.76      0.94      0.84        86\n",
      "\n",
      "avg / total       0.66      0.79      0.72       540\n",
      "\n",
      "LOSS: 87.131\n",
      "Saved model to ../../data/wiki10/model\n",
      "CPU times: user 13min 59s, sys: 6.75 s, total: 14min 6s\n",
      "Wall time: 14min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser', 'tagger', 'ner']) \n",
    "main(nlp, n_iter=2, output_dir=MODEL_DIR, n_texts=1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still getting unstable performance, in some cases there are no predicted case predicted for a specific category. Will need to investigate this further, perhaps it is due to the (incorrect?) loss function in our multi-label model, as we have 6 classes and the correct class only contributes 1/6 to the overall loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
